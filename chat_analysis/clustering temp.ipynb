{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "chat_df = pd.read_csv('chat_data/chat_history 2024-11-08 v3.csv', dtype={\n",
    "    'id': 'int32',\n",
    "    'from_id': 'str',\n",
    "    'text': 'str',\n",
    "    'reply_to_id': 'int32'\n",
    "})\n",
    "chat_df.date = pd.to_datetime(chat_df.date)\n",
    "chat_df.edited = pd.to_datetime(chat_df.edited)\n",
    "chat_df = chat_df[~chat_df.text.str.startswith('File: <')]\n",
    "\n",
    "\n",
    "def load_embeddings(folder, m=0, n=-1):\n",
    "    files = os.listdir(folder)\n",
    "    if not isinstance(m, int) or m < 0 or m >= n:\n",
    "        m = 0\n",
    "        \n",
    "    if not isinstance(n, int) or n < 1 or n > len(files):\n",
    "        n = len(files)\n",
    "    \n",
    "    files = files[m:n]\n",
    "    all_embeds = [torch.load(folder + '/' + files[0])]\n",
    "    for i, t in enumerate(files[1:]):\n",
    "        all_embeds.append(torch.load(folder + '/' + t))\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print((i + 1) * 50)\n",
    "            \n",
    "    return all_embeds\n",
    "\n",
    "\n",
    "embeds = torch.cat(load_embeddings('embeds')).cpu()\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем метки кластеров из результатов применения алгоритмов \n",
    "chat_df = pd.read_csv('chat_data/chat_history 2024-11-08 v3.csv')\n",
    "chat_df = chat_df[~chat_df.text.str.startswith('File: <')]\n",
    "labels = [pd.read_csv('clusters/kmeans_labels1.csv', index_col=0), \n",
    "          pd.read_csv('clusters/spkmeans_labels6.csv', index_col=0), \n",
    "          pd.read_csv('clusters/gmm_labels3.csv', index_col=0),\n",
    "          pd.read_csv('clusters/dbscan_labels7.csv', index_col=0), \n",
    "          pd.read_csv('clusters/hdbscan_labels1.csv', index_col=0)]\n",
    "\n",
    "# Таблица двумерной проекции каждого элемента датасета\n",
    "projections = pd.read_csv('clusters/proj5.csv', index_col=0)\n",
    "chat_df = pd.concat([chat_df, *labels, projections], axis=1)\n",
    "chat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = pd.read_csv('clusters/kmeans_labels1.csv', index_col=0)\n",
    "chat_df['kmeans'] = kmeans_labels\n",
    "# chat_df[['hdbscan']].to_csv('clusters/kmeans_labels1.csv', index=True)\n",
    "\n",
    "cluster_df = chat_df.groupby('kmeans', as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for i, line in cluster_df.iterrows():\n",
    "    print(f'Кластер {line.kmeans} ({len(line.text)})\\n')\n",
    "    for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "        print(str(i + 1) + '. ' + m[:(100 if len(m) > 100 else len(m))], end='\\n')\n",
    "    \n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод слов и их частот для каждого кластера для KMeans\n",
    "from collections import Counter\n",
    "\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "def to_words(messages):\n",
    "    twtk = TweetTokenizer(preserve_case=False)\n",
    "    tokens = []\n",
    "    for m in messages:\n",
    "        tokens.extend(twtk.tokenize(m.lower()))\n",
    "    \n",
    "    # lowercase_tokens = [m.lower() for m in tokens]\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
    "    return list(filter(lambda x: x, filtered_tokens))\n",
    "\n",
    "\n",
    "def make_and_print_clusters(chat_df, labels_col, print_clusters=True):\n",
    "    cluster_df = chat_df.groupby(labels_col, as_index=False).text.apply(list)\n",
    "    cluster_df['size'] = cluster_df.text.apply(len)\n",
    "    cluster_df = cluster_df.sort_values('size', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    cluster_df.text = cluster_df.text.apply(to_words)\n",
    "    # Лемматизация\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "    cluster_df.text = cluster_df.text.apply(lambda x: [\n",
    "        lemmatizer.parse(word)[0].normal_form for word in x])\n",
    "    # Вычисление количества каждого токена в каждом кластере\n",
    "    cluster_df['word_counts'] = cluster_df.text.apply(\n",
    "        lambda wl: sorted([(w, k) for w, k in Counter(wl).items() if k > 1],\n",
    "                          key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    if print_clusters:\n",
    "        # Вывод кластеров\n",
    "        for _, line in cluster_df.iterrows():\n",
    "            print(line[labels_col], f'кластер ({line[\"size\"]})\\n')\n",
    "\n",
    "            for i, (w, k) in enumerate(line.word_counts[:14]):\n",
    "                print(str(i + 1) + '. ' + w[:(100 if len(w) > 100 else len(w))] + f' ({k})', end='\\n')\n",
    "\n",
    "            # for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "            #     print(str(i + 1) + '. ' + m[0][:(100 if len(m[0]) > 100 else len(m[0]))] + f' ({m[1]})', end='\\n')\n",
    "\n",
    "            print('\\n')\n",
    "    \n",
    "    return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = make_and_print_clusters(chat_df, 'kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем облаков слов по кластерам\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def clusters_wordclouds(cluster_df, labels_col, max_words=25, max_font_size=100):\n",
    "    fig, ax = plt.subplots((cluster_df.shape[0] + 3) // 4, 4)\n",
    "    fig.set_figwidth(4 * 6)\n",
    "    fig.set_figheight(((cluster_df.shape[0] + 3) // 4) * 5)\n",
    "    fig.subplots_adjust(0, 0.05, 1, 0.95)\n",
    "    for i, line in cluster_df.iterrows():\n",
    "        text_wordcloud= ' '.join(line.text)\n",
    "        try:\n",
    "            wordcloud = WordCloud(width=600, height=450, max_font_size=max_font_size, max_words=max_words, \n",
    "                                  colormap='Dark2_r', background_color=\"white\").generate(text_wordcloud)\n",
    "        except ValueError:\n",
    "            ax[i // 4, i % 4].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        ax[i // 4, i % 4].imshow(wordcloud)\n",
    "        ax[i // 4, i % 4].set_title(f'Кластер {line[labels_col]} ({line[\"size\"]})', fontsize=24)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "\n",
    "    if cluster_df.shape[0] % 4 != 0:\n",
    "        for i in range(cluster_df.shape[0], ((cluster_df.shape[0] + 3) // 4) * 4):\n",
    "            ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_wordclouds(cluster_df, 'kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ручное объединение кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединение кластеров в группы\n",
    "def group_clusters(df, groups, label_col='kmeans'):\n",
    "    butches = [df[df[label_col].isin(group)] for group in groups]\n",
    "    butches = [group[label_col].apply(lambda _: i) for i, group in enumerate(butches)]\n",
    "    return pd.concat(butches).sort_index()\n",
    "\n",
    "chat_df['new_kmeans'] = group_clusters(chat_df, [\n",
    "        [41, 3, 6, 19, 24, 20, 30, 1], [17, 14, 13, 12, 11, 5], \n",
    "        [2, 35], [39, 25], [31, 27, 23, 29, 18, 37], [7, 9, 22, 26, 28], \n",
    "        [10, 15, 40], [36], [21], [4], [33, 8, 38], [0, 16, 32, 34]], \n",
    "    'kmeans')\n",
    "cluster_names = {...}  # Названия кластеров по их номерам в списке объединённых кластеров выше\n",
    "chat_df['new_kmeans'] = chat_df['new_kmeans'].replace(cluster_names)\n",
    "chat_df[['kmeans', 'new_kmeans']].to_csv('clusters/kmeans_labels1.csv', index=True)\n",
    "print(chat_df['new_kmeans'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод сообщений в объединённых кластерах\n",
    "cluster_df = chat_df.groupby('new_kmeans', as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for i, line in cluster_df.iterrows():\n",
    "    print(f'Кластер {line.new_kmeans} ({len(line.text)})\\n')\n",
    "    for i, m in enumerate(line.text[:(20 if len(line.text) > 20 else len(line.text))]):\n",
    "        print(str(i + 1) + '. ' + m[:(100 if len(m) > 100 else len(m))], end='\\n')\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод слов (токенов) в объединённых кластерах\n",
    "cluster_df = make_and_print_clusters(chat_df, 'new_kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем топ слова кластеров после объединения\n",
    "clusters_wordclouds(cluster_df, 'new_kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def visualize_clusters(chat_df, labels_col, title, file: str | None = None):\n",
    "    # Визуализация\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "    fig.set_figwidth(100)\n",
    "    fig.set_figheight(100)\n",
    "    fig.suptitle(title, fontsize=120)\n",
    "\n",
    "    colors = cm.get_cmap('hsv', len(chat_df[labels_col].unique()))\n",
    "    for i, proj in enumerate(['pca', 'tsne', 'pca_tsne', 'umap']):\n",
    "        for j, c in enumerate(chat_df[labels_col].unique()):\n",
    "            ax[i // 2, i % 2].scatter(chat_df[chat_df[labels_col] == c][proj + 'X'], \n",
    "                                      chat_df[chat_df[labels_col] == c][proj + 'Y'], \n",
    "                                      color=colors(j), label=f'{c}')\n",
    "\n",
    "        ax[i // 2, i % 2].set_title(proj, fontsize=100)\n",
    "\n",
    "    # Получаем элементы легенды из первого подграфика\n",
    "    handles, labels = ax[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper left', ncol=1, fontsize=75, markerscale=7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if file is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(chat_df, 'new_kmeans', 'KMeans', 'clusters/kmeans_proj1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сферический k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from soyclustering import SphericalKMeans\n",
    "\n",
    "\n",
    "spherical_kmeans = SphericalKMeans(\n",
    "    n_clusters=144,\n",
    "    max_iter=10,\n",
    "    verbose=1,\n",
    "    init='similar_cut',\n",
    "    sparsity='minimum_df',\n",
    "    minimum_df_factor=0.02\n",
    ")\n",
    "\n",
    "spkmeans_labels = spherical_kmeans.fit_predict(csr_matrix(embeds.numpy()))\n",
    "# silhouette_score inertia / n_clusters max_iters\n",
    "# 0.017295146 7943.604631717489 / 250 20\n",
    "# 0.026064537 8099.005042571704 / 150 10 \n",
    "# 0.0282372 7888.141710191523 / 138 10\n",
    "# 0.02712679 7956.118294731778 / 125 10\n",
    "# 0.023679893 8267.833774735958 / 100 10\n",
    "# 0.01999612 8501.070381560368 / 50 10\n",
    "print(silhouette_score(embeds, spherical_kmeans.labels_), \n",
    "      davies_bouldin_score(embeds, spherical_kmeans.labels_), \n",
    "      calinski_harabasz_score(embeds, spherical_kmeans.labels_), \n",
    "      spherical_kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soyclustering import merge_close_clusters, visualize_pairwise_distance\n",
    "\n",
    "# вывод результатов объединения кластеров и графика попарных расстояний между ними\n",
    "def print_groups(centers, labels, max_dist):\n",
    "    group_centers, groups = merge_close_clusters(centers, labels, max_dist=max_dist)\n",
    "    for group in groups:\n",
    "        print(list(sorted(group)))\n",
    "\n",
    "    visualize_pairwise_distance(group_centers, max_dist=max_dist, sort=True)\n",
    "    return group_centers, groups\n",
    "\n",
    "gc1, gs1 = print_groups(spherical_kmeans.cluster_centers_, spherical_kmeans.labels_, .03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавление результатов кластеризации в датафрейм и вывод кластеров\n",
    "chat_df['spkmeans'] = spherical_kmeans.labels_\n",
    "cluster_df = chat_df.groupby('spkmeans', as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for i, line in cluster_df.iterrows():\n",
    "    print(line.spkmeans, f'кластер ({len(line.text)})\\n')\n",
    "    for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "        print(str(i + 1) + '. ' + m[:(100 if len(m) > 100 else len(m))], end='\\n')\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединение кластеров в группы\n",
    "chat_df['new_spkmeans'] = group_clusters(chat_df, gs1, 'spkmeans')\n",
    "print(chat_df['new_spkmeans'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод кластеров после объединения \n",
    "cluster_df = make_and_print_clusters(chat_df, 'new_spkmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем топ слова кластеров\n",
    "clusters_wordclouds(cluster_df, 'new_spkmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод кластеров после объединения (с выводом значимых слов через TF-IDF)\n",
    "from collections import Counter\n",
    "\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "labels_col = 'spkmeans'\n",
    "\n",
    "\n",
    "def collect_lists(*args):\n",
    "    all_els = []\n",
    "    for l in args:\n",
    "        arg_list = []\n",
    "        for el in l:\n",
    "            arg_list.extend(el)\n",
    "        \n",
    "        all_els.extend(arg_list)\n",
    "    \n",
    "    return all_els\n",
    "\n",
    "\n",
    "cluster_df = chat_df.groupby(labels_col, as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False).reset_index(drop=True)\n",
    "\n",
    "cluster_df.text = cluster_df.text.apply(to_words)\n",
    "lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "cluster_df.text = cluster_df.text.apply(lambda x: [lemmatizer.parse(word)[0].normal_form for word in x])\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, decode_error='ignore', \n",
    "                             vocabulary=set(collect_lists(cluster_df.text.to_list())))\n",
    "tfidf_matrix = vectorizer.fit_transform(cluster_df.text)\n",
    "fnames = vectorizer.get_feature_names_out()\n",
    "\n",
    "cluster_df.text = cluster_df.text.apply(lambda wl: sorted([(w, k) for w, k in Counter(wl).items() if k > 1], \n",
    "                                                          key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# cluster_df['size'] = cluster_df.text.apply(len)\n",
    "# cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for _, line in cluster_df.iterrows():\n",
    "    print(line[labels_col], f'кластер ({line[\"size\"]})\\n')\n",
    "\n",
    "    tfidf_vector = tfidf_matrix[line[labels_col]].toarray()[0]\n",
    "    word_tfidf = {fnames[i]: tfidf_vector[i] for i in range(len(tfidf_vector)) \n",
    "                  if tfidf_vector[i] > 0 and not fnames[i].isdigit()}\n",
    "    word_tfidf = sorted(word_tfidf.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    # print([w for w, _ in line.text if not any(w == word for word in fnames)])\n",
    "    # print([(w, k) for w, _ in word_tfidf for word, k in line.text if word == w])\n",
    "    word_tfidf = sorted([(w, k) for w, _ in word_tfidf for word, k in line.text if word == w], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # print(word_tfidf)\n",
    "    # print(line.text)\n",
    "    for i, (w, k) in enumerate(word_tfidf):\n",
    "        # frequancy = [k for word, k in line.text if word == w]\n",
    "        # print(frequancy)\n",
    "        print(str(i + 1) + '. ' + w[:(100 if len(w) > 100 else len(w))] + f' ({k})', end='\\n')\n",
    "\n",
    "    # for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "    #     print(str(i + 1) + '. ' + m[0][:(100 if len(m[0]) > 100 else len(m[0]))] + f' ({m[1]})', end='\\n')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вторая группировка\n",
    "gc2, gs2 = print_groups(gc1, chat_df.new_spkmeans, 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранение результатов\n",
    "chat_df[['spkmeans', 'new_spkmeans']].to_csv('clusters/spkmeans_labels6.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# Сокращение размерности\n",
    "pca = PCA(n_components=2, whiten=False)\n",
    "pca_alt = PCA(n_components=128, whiten=False)\n",
    "tsne = TSNE(n_components=2, perplexity=50, early_exaggeration=24, metric='cosine', n_iter=2000, n_jobs=-1, verbose=True)\n",
    "reducer = umap.UMAP(n_neighbors=8, n_components=2, metric='cosine', n_epochs=200, \n",
    "                    learning_rate=1., min_dist=0.01, spread=1, low_memory=False, n_jobs=-1, verbose=True)\n",
    "\n",
    "pca_embeds = pca.fit_transform(embeds)\n",
    "print('PCA embeddings', pca_embeds.shape)\n",
    "\n",
    "tsne_embeds = tsne.fit_transform(embeds)\n",
    "print('t-SNE embeddings', tsne_embeds.shape)\n",
    "\n",
    "pca_embeds2 = pca_alt.fit_transform(embeds)\n",
    "pca_tsne_embeds = tsne.fit_transform(pca_embeds2)\n",
    "print('PCA + t-SNE embeddings', pca_tsne_embeds.shape)\n",
    "\n",
    "umap_embeds = reducer.fit_transform(embeds)\n",
    "\n",
    "print('UMAP embeddings', umap_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# pca = PCA(64, whiten=False)\n",
    "reducer = umap.UMAP(n_neighbors=8, n_components=2, metric='cosine', n_epochs=250, init='pca',\n",
    "                    learning_rate=1., min_dist=0.01, spread=12, low_memory=False, n_jobs=-1, verbose=True)\n",
    "umap_embeds = reducer.fit_transform(embeds)\n",
    "print('UMAP embeddings', umap_embeds.shape)\n",
    "\n",
    "projs = pd.read_csv('clusters/proj4.csv', index_col=0)\n",
    "projs['umapX'] = umap_embeds[:, 0]\n",
    "projs['umapY'] = umap_embeds[:, 1]\n",
    "projs.to_csv('clusters/proj5.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df['pcaX'] = pca_embeds[:, 0]\n",
    "chat_df['pcaY'] = pca_embeds[:, 1]\n",
    "chat_df['tsneX'] = tsne_embeds[:, 0]\n",
    "chat_df['tsneY'] = tsne_embeds[:, 1]\n",
    "chat_df['pca_tsneX'] = pca_tsne_embeds[:, 0]\n",
    "chat_df['pca_tsneY'] = pca_tsne_embeds[:, 1]\n",
    "chat_df['umapX'] = umap_embeds[:, 0]\n",
    "chat_df['umapY'] = umap_embeds[:, 1]\n",
    "chat_df[['pcaX', 'pcaY', 'tsneX', 'tsneY', 'pca_tsneX', 'pca_tsneY', 'umapX', 'umapY']].to_csv('clusters/proj4.csv', index=True)\n",
    "chat_df[['pcaX', 'pcaY', 'tsneX', 'tsneY', 'pca_tsneX', 'pca_tsneY', 'umapX', 'umapY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(chat_df, 'new_spkmeans', 'Spherical KMeans', 'clusters/spkmeans_proj4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Смесь гауссиан"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "gaussmix = GaussianMixture(35, covariance_type='tied', n_init=1, verbose=True, verbose_interval=10)\n",
    "gaussmix_labels = gaussmix.fit_predict(embeds)\n",
    "# silhouette davies_bouldin / n_components covariance_type\n",
    "# 0.014587272 3.9356715807503546 / 5 tied\n",
    "# -0.0074547827 3.7814189379906167 / 10 spherical\n",
    "# -0.0031154878 3.9669980278232053 / 20 diag\n",
    "# 0.017083913 3.893883616755334 / 50 full\n",
    "\n",
    "print(silhouette_score(embeds, gaussmix_labels), \n",
    "      davies_bouldin_score(embeds, gaussmix_labels), \n",
    "      calinski_harabasz_score(embeds, gaussmix_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавление результатов кластеризации в датафрейм и вывод кластеров\n",
    "# chat_df['gmm'] = gaussmix_labels\n",
    "cluster_df = chat_df.groupby('gmm', as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for i, line in cluster_df.iterrows():\n",
    "    print(line.gmm, f'кластер ({len(line.text)})\\n')\n",
    "    for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "        print(str(i + 1) + '. ' + m[:(100 if len(m) > 100 else len(m))], end='\\n')\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "# chat_df[['gmm']].to_csv('clusters/gmm_labels3.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод кластеров после объединения\n",
    "cluster_df = make_and_print_clusters(chat_df, 'gmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_wordclouds(cluster_df, 'gmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(chat_df, 'gmm', 'Gaussian Mixture Model', 'clusters/gmm_proj3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def k_mean_dist(idx, points, k):\n",
    "    distances = []\n",
    "    for i in range(0, points.shape[0] - 1000, 1000):\n",
    "        distances.extend(Parallel(n_jobs=-1)(delayed(lambda a, b: np.linalg.norm(a - b, 2))(points[idx], points[j]) \n",
    "                                             for j in range(i, i + 1000)))\n",
    "    \n",
    "    distances.extend(Parallel(n_jobs=-1)(delayed(lambda a, b: np.linalg.norm(a - b, 2))(points[idx], points[j]) \n",
    "                                         for j in range(i, points.shape[0])))\n",
    "    distances.sort()\n",
    "    return sum(distances[1:k+1]) / k\n",
    "\n",
    "\n",
    "mean_distances = []\n",
    "try:\n",
    "    for i in range(10000, embeds.shape[0]):\n",
    "        mean_distances.append(k_mean_dist(i, embeds, 4))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "mean_distances.sort()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(len(mean_distances)), mean_distances)\n",
    "plt.ylabel('Среднее расстояние до ближайших 4-х соседей')\n",
    "plt.savefig('clusters/dbscan_mean_distances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(0.4, min_samples=4, metric='euclidean', algorithm='ball_tree', n_jobs=-1)\n",
    "\n",
    "dbscan_labels = dbscan.fit_predict(embeds)\n",
    "print('algorithm has converged')\n",
    "# silhouette davies calinski / eps min_samples\n",
    "# -0.08605397 1.0740822871715672 30.563701039150505 / 0.1 8\n",
    "# -0.14635736 1.6975759440481935 38.94185852977294 / 0.3 8\n",
    "# 0.029522894 1.9106617470272766 50.07569050431296 / 0.435 4\n",
    "# -0.038184512 1.8158209459582089 42.25380106716254 / 0.4 4\n",
    "# -0.07685446 1.844907477428522 36.94501188877512 / 0.38 4\n",
    "print(silhouette_score(embeds, dbscan_labels), \n",
    "      davies_bouldin_score(embeds, dbscan_labels), \n",
    "      calinski_harabasz_score(embeds, dbscan_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для уже полученных меток\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "dbscan_labels = pd.read_csv('clusters/dbscan_labels7.csv', index_col=0)\n",
    "print(silhouette_score(embeds, dbscan_labels), \n",
    "      davies_bouldin_score(embeds, dbscan_labels), \n",
    "      calinski_harabasz_score(embeds, dbscan_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавление результатов кластеризации в датафрейм и вывод кластеров\n",
    "dbscan_labels = pd.read_csv('clusters/dbscan_labels2.csv', index_col=0)\n",
    "chat_df['dbscan'] = dbscan_labels\n",
    "cluster_df = chat_df.groupby('dbscan', as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for i, line in cluster_df.iterrows():\n",
    "    print(f'Кластер {line.dbscan} ({len(line.text)})\\n')\n",
    "    for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "        print(str(i + 1) + '. ' + m[:(100 if len(m) > 100 else len(m))], end='\\n')\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "# chat_df[['dbscan']].to_csv('clusters/dbscan_labels8.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df.dbscan.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод кластеров \n",
    "make_and_print_clusters(chat_df, 'dbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация\n",
    "visualize_clusters(chat_df, 'dbscan', 'DBSCAN', 'clusters/dbscan_proj7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "\n",
    "hdbscan = HDBSCAN(min_samples=34, xi=0.05, min_cluster_size=0.05, algorithm='ball_tree', n_jobs=-1)\n",
    "hdbscan_labels = hdbscan.fit_predict(embeds)\n",
    "print('algorithm has converged')\n",
    "# \n",
    "print(silhouette_score(embeds, hdbscan_labels), \n",
    "      davies_bouldin_score(embeds, hdbscan_labels), \n",
    "      calinski_harabasz_score(embeds, hdbscan_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_labels = pd.read_csv('clusters/hdbscan_labels1.csv', index_col=0)\n",
    "chat_df['hdbscan'] = hdbscan_labels\n",
    "# chat_df[['hdbscan']].to_csv('clusters/hdbscan_labels1.csv', index=True)\n",
    "\n",
    "cluster_df = chat_df.groupby('hdbscan', as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for i, line in cluster_df.iterrows():\n",
    "    print(f'Кластер {line.hdbscan} ({len(line.text)})\\n')\n",
    "    for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "        print(str(i + 1) + '. ' + m[:(100 if len(m) > 100 else len(m))], end='\\n')\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация\n",
    "visualize_clusters(chat_df, 'hdbscan', 'HDBSCAN', 'clusters/hdbscan_proj4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Агломеративная кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggl_labels = pd.read_csv('clusters/aggl_labels1.csv', index_col=0)\n",
    "chat_df['aggl'] = aggl_labels\n",
    "# chat_df[['hdbscan']].to_csv('clusters/hdbscan_labels1.csv', index=True)\n",
    "\n",
    "cluster_df = chat_df.groupby('aggl', as_index=False).text.apply(list)\n",
    "cluster_df['size'] = cluster_df.text.apply(len)\n",
    "cluster_df = cluster_df.sort_values('size', ascending=False)\n",
    "for i, line in cluster_df.iterrows():\n",
    "    print(f'Кластер {line.aggl} ({len(line.text)})\\n')\n",
    "    for i, m in enumerate(line.text[:(10 if len(line.text) > 10 else len(line.text))]):\n",
    "        print(str(i + 1) + '. ' + m[:(100 if len(m) > 100 else len(m))], end='\\n')\n",
    "    \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "\n",
    "linked = linkage(embeds[:10000, :], method='ward')\n",
    "dendrogram(fcluster(linked, 100, criterion='maxclust'), orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
